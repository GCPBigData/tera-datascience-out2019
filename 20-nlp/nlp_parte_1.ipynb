{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula #26 – Processamento de Linguagem Natural & Análise de Sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "NLP (ou _Natural Language Processing_ - a sigla em português é PLN, _Processamento de Linguagem Natural_, mas essa sigla também é usada para falar de _Programação Neurolinguística_, então, vamos continuar usando NLP, ok?) é a área relacionada a técnicas para o entendimento de linguagem humana (a linguagem natural). A aplicação de técnicas de NLP tem sido vista em muitos domínios, desde a correção de palavras e sugestão de palavras na barra de busca de sites como o _Google_, até em sistemas mais \"sofisticados\" como os tradutores automáticos e os _home assistant_ e _smart assistants_, como Siri, Alexa ou Google Home, que podem auxiliar na execução de certas tarefas, como agendar compromissos, embora eles ainda [estejam longe de ser à prova de erros](https://www.nytimes.com/2018/05/25/business/amazon-alexa-conversation-shared-echo.html). Com o crescimento de conteúdo (mais artigos e mais fontes espalhadas), a importância do NLP também tem crescido, pois tarefas como sumarização automática e classificação automática de conteúdo (por ex., para checagem contra _fake news_) são cada vez mais necessárias em nossas vidas.\n",
    "\n",
    "Nesta aula, vamos focar no uso de algumas técnicas de NLP para processamento de texto. Entretanto, é importante lembrar que NLP não se restringe a textos escritos, podendo ser aplicado também para processamento de fala, como é o caso dos _smart assistants_.\n",
    "\n",
    "Diferentes tarefas de NLP, em geral, têm como passos iniciais as seguintes etapas:\n",
    "\n",
    "* pré-processamento do texto (que pode ser uma combinação de diferentes processamentos, envolvendo modificações a nível de palavra e identificação de entidades/funções sintáticas/etc.)\n",
    "\n",
    "* transformação do texto em quantidades numéricas (tipicamente vetores de números inteiros ou reais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda de hoje\n",
    "\n",
    "* análise de sentimento (detecção de polaridade)\n",
    "\n",
    "* ferramenta para encontrar ingredientes que combinam (usando _embeddings_ criados por _word2vec_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de sentimento\n",
    "\n",
    "Esse conjunto de dados é composto de reviews de produtos coletados do site `Americanas.com` entre janeiro e maio de 2018.\n",
    "\n",
    "Mais informações podem ser encontradas no [repositório b2w-reviews01](https://github.com/b2wdigital/b2w-reviews01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -P data/datasets/b2w_reviews https://github.com/b2wdigital/b2w-reviews01/raw/master/B2W-Reviews01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['product_name', 'product_brand', 'site_category_lv1', 'site_category_lv2',\n",
    "                 'review_title', 'review_text', 'recommend_to_a_friend']\n",
    "\n",
    "path = 'data/datasets'\n",
    "reviews_df = pd.read_csv(f'{path}/b2w_reviews/B2W-Reviews01.csv', sep=';', low_memory=False)[selected_cols].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['text'] = reviews_df['review_title'] + ' ' + reviews_df['review_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df[reviews_df['text'].str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['recommend_to_a_friend'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df[['text', 'recommend_to_a_friend']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, tanto no dataset de treino, como no de teste, temos o texto e polaridade da sentença. Nosso objetivo é construir um **classificador de sentimentos**, que recebe uma sentença (referente a um review de produto) e é capaz de predizer se o review é positivo (o usuário recomendaria a um amigo) ou negativo (o usuário **não** recomendaria a um amigo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.sample(n=10)['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nos exemplos de frases acima, podemos ver que as sentenças incluem pontuações, acentos, letras maiúsculas e minúsculas... Seria ideal que conseguíssemos _normalizar_ o texto, de forma a diminuir a quantidade de palavras diferentes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização do texto\n",
    "\n",
    "Técnicas comuns:\n",
    "\n",
    "* remoção de acentos\n",
    "\n",
    "* remoção de palavras muito comuns (_stopwords_)\n",
    "\n",
    "* remoção de pontuação\n",
    "\n",
    "* remoção de dígitos\n",
    "\n",
    "* padronização para letras minúsculas (ou maiúsculas)\n",
    "\n",
    "* _stemming_ / _lematização_ - redução de palavras relacionadas a uma forma mínima comum (ex. `construirá -> construir`, `construção -> construir`)\n",
    "\n",
    "* correção de palavras escritas incorretamente (uso de spellchecker)\n",
    "\n",
    "**Leia mais:**\n",
    "\n",
    "* sobre a diferença entre stemming e lematização em [uma discussão no StackOverflow](https://stackoverflow.com/questions/1787110/what-is-the-true-difference-between-lemmatization-vs-stemming);\n",
    "\n",
    "* sobre a situação atual em relação à lematização para a língua portuguesa nesse [blog post](https://lars76.github.io/nlp/lemmatize-portuguese/).\n",
    "\n",
    "**Atenção:** todas essas técnicas envolvem operações com `strings`. Como estamos trabalhando com um dataframe Pandas, quando possível, vamos usar os métodos descritos [na documentação do Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção de acentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['norm_text'] = reviews_df['text'].str.normalize('NFKD').str.encode('ascii', 'ignore').str.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forma alternativa:\n",
    "\n",
    ">```python\n",
    "> from unicodedata import normalize\n",
    ">\n",
    "> def remove_accents(text):\n",
    "    return normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    ">\n",
    "> reviews_df['norm_text'] = reviews_df['text'].apply(normalize_text)\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df[['text', 'norm_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padronização para letras minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['norm_text'] = reviews_df['norm_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df[['text', 'norm_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção de dígitos\n",
    "\n",
    "Aqui, para facilitar, vamos usar expressões regulares, um tópico que estava no `Pré-aula`.\n",
    "\n",
    "Para relembrar: https://regexone.com/lesson/letters_and_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['norm_text'] = reviews_df['norm_text'].str.replace(r'[0-9]', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forma alternativa **sem** usar expressões regulares:\n",
    "\n",
    "> ```python\n",
    "> for digit in range(10):\n",
    "    reviews_df['norm_text'] = reviews_df['norm_text'].str.replace(str(digit), '')\n",
    "> ``` \n",
    "\n",
    "Forma alternativa usando expressões regulares, explicitamente usando a biblioteca [re](https://docs.python.org/3.6/library/re.html):\n",
    "\n",
    "> ```python\n",
    "> import re\n",
    ">\n",
    "> def replace_digits(text):\n",
    "    return re.sub(r'\\d', '', text)\n",
    "> \n",
    "> reviews_df['norm_text'] = reviews_df['norm_text'].apply(replace_digits)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df[reviews_df['text'].str.contains('0')][['text', 'norm_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção de pontuação\n",
    "\n",
    "Para remover a pontuação, podemos usar o próprio módulo `string` do python, que já tem mapeadas as pontuações de texto possíveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "translation_table = str.maketrans({key: ' ' for key in string.punctuation}) \n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(translation_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vamos testar a função?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quando ela olhou, gritei bem alto: \\n - \"Não me engana, não, hein?!\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation('Quando ela olhou, gritei bem alto: \\n - \"Não me engana, não, hein?!\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['norm_text'] = reviews_df['norm_text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df[reviews_df['text'].str.contains('!')][['text', 'norm_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' / '.join(nltk.corpus.stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Ponto importante_:** Como nossa tarefa é uma análise de sentimentos, seria muito ruim perder certas _stopwords_ como palavras de negação, afinal, \"Eu **não** gosto disso\"  é muito diferente de \"Eu gosto disso\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, vamos manter algumas das palavras que podem ser essenciais para a classificação:\n",
    "\n",
    "* `não`\n",
    "\n",
    "* `mas`\n",
    "\n",
    "Ou seja, gostaríamos de remover as palavras `não` e `mas` da lista de stopwords fornecida pelo `nltk`. Além disso, note que essa lista contém acentos. Vamos retirá-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def remove_accents(text):\n",
    "    return normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa:** \n",
    "\n",
    "Crie uma lista chamada `stopwords` que contém todas as palavras de `nltk.corpus.stopwords.words('portuguese')`, exceto as palavras `não` e `mas`. A cada palavra deve ser aplicada a função `normalize_text`.\n",
    "\n",
    "1. defina uma lista chamada `not_allowed` com as palavras \"não\" e \"mas\".\n",
    "2. crie uma lista vazia chamada stopwords\n",
    "3. faça um loop para percorrer cada uma das palavras de `nltk.corpus.stopwords.words('portuguese')`\n",
    "    * a cada loop, você deve checar se a palavra atual está contida dentro de `not_allowed`\n",
    "    * se **não** estiver, você deve incluir a palavra à lista de stopwords\n",
    "\n",
    "<!-- \n",
    "not_allowed = [\"não\", \"mas\"]\n",
    "stopwords = []\n",
    "for word in nltk.corpus.stopwords.words('portuguese'):\n",
    "    if word not in not_allowed:\n",
    "        stopwords.append(remove_accents(word))\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. \n",
    "# not_allowed = [###]\n",
    "\n",
    "# # 2.\n",
    "# stopwords = ###\n",
    "\n",
    "# # 3.\n",
    "# for ###\n",
    "#     if ###:\n",
    "#         stopwords.append(###)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' / '.join(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* note que há palavras repetidas, por termos removido os acentos. Vamos retirá-las transformando `stopwords` em um _set_ (uma estrutura de dados de conjunto, que automaticamente remove repetições)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' / '.join(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* salvando o dataset para uso futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.to_csv('data/datasets/b2w_reviews/normalized_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo (finalmente!!!)\n",
    "\n",
    "Finalmente, chegamos à parte de treinar o modelo de análise de sentimentos!!!\n",
    "\n",
    "Ou quase... na verdade, antes de treinar o modelo, precisamos transformar o texto em _features_ numéricas.\n",
    "\n",
    "A maneira mais simples de transformar um texto em um vetor de números é usando o método comumente chamado de _Bag of words_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_for_bow = [\n",
    "    'camisa preta',\n",
    "    'botao feito linha preta',\n",
    "    'considera-se caro preco botao camisa botao',\n",
    "    'linha costurar botão mesma camisa',\n",
    "    'costurar linha camisa mesma botao'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=5, strip_accents='unicode', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix = cv.fit_transform(examples_for_bow)\n",
    "bow_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bow_matrix.todense(), columns=sorted(cv.vocabulary_.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que os exemplos `3` e `4` têm a mesma representação numérica, mesmo que a ordem das palavras não seja a mesma! Essa é uma característica desse método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão de dataset em treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa:** Divida o dataframe `reviews_df` em um dataframe de treino (`train_df`) e um de teste (`test_df`)\n",
    "\n",
    "1. crie uma variável chamada `target_vals` com os valores da coluna `target` em uma lista\n",
    "2. use a função do sklearn `train_test_split` para dividir o dataframe. Lembre-se de usar o parâmetro `stratify`, passandro para ele a variável `target_vals`.\n",
    "    \n",
    "<!-- \n",
    "target_vals = reviews_df['recommend_to_a_friend'].values\n",
    "train_df, test_df = train_test_split(reviews_df, test_size=0.2, stratify=target_vals)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "target_vals = ###\n",
    "\n",
    "# 2\n",
    "train_df, test_df = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['norm_text', 'recommend_to_a_friend']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['norm_text', 'recommend_to_a_friend']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['y'] = (train_df['recommend_to_a_friend'] == 'Yes').astype(int)\n",
    "test_df['y'] = (test_df['recommend_to_a_friend'] == 'Yes').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treino do classificador Decision Tree\n",
    "\n",
    "Uma vantagem de usar a árvore de decisão é que ela é interpretável: ao final do treino, poderemos ver quais são as palavras que mais importam e explorar um pouco as regras de decisão para cada classificação.\n",
    "\n",
    "**Leia mais:** Se quiser um exemplo usando outro tipo de algoritmo, você pode ver esse [post no Medium](https://medium.com/@minbaekim/text-mining-preprocess-and-naive-bayes-classifier-da0000f633b2). \n",
    "\n",
    "**Aprofunde-se:** Veja a diferença entre utilizar SVM (Support Vector Machines) e Decision Trees para classificação de texto [aqui](https://www.codementor.io/blog/text-classification-6mmol0q8oj)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('vect', CountVectorizer(max_features=200, stop_words=stopwords)),\n",
    "    ('clf', DecisionTreeClassifier(min_samples_leaf=10, class_weight='balanced'))\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['norm_text'].values\n",
    "y_train = train_df['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa:** Teste seu classificador usando um texto de exemplo. Veja que você pode usar tanto o método `predict` como o método `predict_proba`.\n",
    "\n",
    "1. crie uma variável `text` com qualquer texto ou com o texto de uma das avaliações do dataset\n",
    "2. faça uma chamada com `sentiment_analyzer.predict` ou `sentiment_analyzer.predict_proba` passando como parâmetro uma lista contendo a variável `text`: `[text]`\n",
    "\n",
    "\n",
    "<!-- \n",
    "text = test_df.iloc[0]['norm_text']\n",
    "sentiment_analyzer.predict([text])\n",
    "sentiment_analyzer.predict_proba([text])\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "text = ###\n",
    "\n",
    "# 2\n",
    "sentiment_analyzer.###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotando as features mais importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = sentiment_analyzer.named_steps['vect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = sorted(zip(features, sentiment_analyzer.named_steps['clf'].feature_importances_), key=lambda elem: elem[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.axisbelow'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5), dpi=120)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.barh(*zip(*sorted_features[:20][::-1]), color='skyblue', edgecolor='w')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.grid()\n",
    "plt.title('Top 10 features mais importantes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* profundidade da árvore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer.named_steps['clf'].get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* note o parâmetro `max_depth=2`, que indica que vamos plotar apenas até o nível de profundidade 2 da árvore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6), dpi=120)\n",
    "ax = fig.add_subplot(111)\n",
    "_ = tree.plot_tree(sentiment_analyzer.named_steps['clf'], max_depth=2, label='root', proportion=False,\n",
    "    feature_names=features, class_names=['not_recommend', 'recommend'], ax=ax, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação do classificador\n",
    "\n",
    "**Tarefa:** Faça a predição da coluna `norm_text` e compare o resultado com o vetor target (coluna `y`).\n",
    "\n",
    "1. crie a variável `X_test` (dica: veja como criamos a variável `X_train`. Deve ser a mesma coisa, utilizando agora o dataframe `test_df`)\n",
    "2. crie a variável `y_test` (dica: veja como criamos a variável `y_train`. Deve ser a mesma coisa, utilizando agora o dataframe `test_df`)\n",
    "3. crie a variável `y_pred` com as predições do modelo `sentiment_analyzer` em `X_train`\n",
    "4. imprima o [classification_report](https://scikit-learn.org/0.19/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) usando como parâmetros `y_test`, `y_pred` e como `target_names` uma lista `['positive', 'negative']`.\n",
    "\n",
    "<!-- \n",
    "X_test = test_df['norm_text'].values\n",
    "y_test = test_df['y'].values\n",
    "y_pred = sentiment_analyzer.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'positive']))\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "X_test = ###\n",
    "# 2\n",
    "y_test = ###\n",
    "# 3\n",
    "y_pred = ###\n",
    "# 4\n",
    "print(classification_report(###))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* matriz de confusão (veja documentação do [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html))\n",
    "\n",
    "|      | pred_0| pred_1|\n",
    "|------|-------|-------|\n",
    "|   0  |  TN   |  FP   |\n",
    "|   1  |  FN   |  TP   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test, y_pred), columns=['pred_0', 'pred_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pergunta final:** O que você achou do classificador? Ele é bom ou ruim?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais considerações e possíveis modificações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que após aplicar o _Bag of words_, também poderíamos ter aplicado a transformação Tfidf, que discrimina as palavras de acordo com a \"relevância\" delas em cada documento em relação ao _corpus_ (i.e. o conjunto total de documentos).\n",
    "\n",
    "**Tarefa Bônus:** Inclua no _pipeline_ a transformação `Tfidf` e compare os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos que nosso classificador possui tanto o método `predict` como o método `predict_proba`. Ao usar o método `predict_proba`, temos as probabilidades de que o texto seja `positive` ou `negative`. Podemos escolher um _threshold_ de forma a maximizar uma das métricas.\n",
    "\n",
    "**Tarefa Bônus:** Escolha esse threshold de forma a maximizar nosso `recall` de exemplos positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra alternativa seria processar o texto de uma maneira diferente: imagine que só quiséssemos incluir três tipos de classes gramaticais: verbo, adjetivo e advérbio, que normalmente são as classes que indicam o sentimento de um indivíduo em relação a alguma coisa.\n",
    "\n",
    "Um módulo bastante utilizado em NLP é o [spacy](https://spacy.io/). No `spacy`, é possível treinar um modelo para que ele reconheça certas estruturas comuns em textos de uma determinada língua. No caso do inglês (e também do português), podemos baixar o modelo e começar a usar!\n",
    "\n",
    "Além de ser bem completo, ele costuma ser mais rápido que o módulo \"concorrente\" `nltk`. [Esse benchmark](https://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/) compara três tarefas (tokenização de palavras, tokenização de sentenças e classificação gramatical - comumente chamado de PoS tag, _part-of-speech tag_), que é o que queremos fazer.\n",
    "\n",
    "No modelo do `spacy`, é possível identificar os seguintes tipos de PoS tag:\n",
    "\n",
    "* ADJ\n",
    "* ADP\n",
    "* ADV\n",
    "* CONJ\n",
    "* DET\n",
    "* INTJ\n",
    "* NOUN\n",
    "* PART (e.g. possessive marker _'s_)\n",
    "* PRON\n",
    "* PROPN\n",
    "* PUNCT\n",
    "* SPACE\n",
    "* SYM\n",
    "* VERB\n",
    "* X (unknown)\n",
    "\n",
    "**Obs.:** na verdade isso vale para o _coarse-grained PoS tag_ (que é visto chamando o atributo `pos` de um objeto `Token`), para o _fine-grained PoS tag_, há muito [mais classificações disponíveis](https://spacy.io/api/annotation).\n",
    "\n",
    "**Usando o spacy**\n",
    "\n",
    "Ao fazer uma chamada do tipo `nlp(text)`, é aplicado o pipeline composto pelo `tagger` (PoS tag), `parser` (_parseador_ de dependências) e `ner` (reconhecimento de entidades) no texto. O retorno é um objeto do tipo `Doc`, que é uma sequência de objetos `Token`. Cada uma dos objetos `Token` terá o atributo `pos_`, que trará a classe gramatical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Ótimo produto, porém embalagem péssima. O \"saco\" que veio o tênis chegou todo rasgado por causa da pessima embalagem.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, se imprimíssemos somente as palavras de classe `ADJ`, `ADV` e `VERB`, teríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_pos = set(['ADJ', 'ADV', 'VERB'])\n",
    "for token in doc:\n",
    "    if token.pos_ not in allowed_pos:\n",
    "        continue\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, ele não acerta tudo, mas já pode ser suficiente para nós.\n",
    "\n",
    "Uma alternativa ao `spacy` é usar a biblioteca [nlpnet](https://github.com/erickrf/nlpnet) para extrair as classes gramaticais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa Bônus:** Filtre os adjetivos, advérbios e verbos do texto e construa um novo modelo que use esse novo texto como input e avalie o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
