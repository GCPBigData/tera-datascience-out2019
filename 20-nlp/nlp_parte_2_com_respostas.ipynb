{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula #26 – Processamento de Linguagem Natural & Análise de Sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "Já vimos antes que é possível transformar um texto em _features_ numéricas. Uma sofisticação do método _Bag of words_ é incorporar o contexto das palavras vizinhas nessas _features_ (é comum chamar o vetor de _features_ numéricas de _embedding_).\n",
    "\n",
    "Imagine que nossa janela de contexto (context window) tem tamanho 5 (2 palavras _antes_ e 2 palavras _depois_ da palavra _central_).\n",
    "\n",
    "Então, se a frase fosse `The quick brown fox jumps over the lazy dog`, teríamos as seguintes janelas:\n",
    "\n",
    "<img src=\"data/nb_figs/windows_word2vec.png\" width=\"600\"/>\n",
    "\n",
    "Para cada uma das janelas formadas, temos o vetor correspondente a elas (usando o _Bag of words_ binário - com apenas 0s e 1s; também chamado de `one-hot encoding`):\n",
    "\n",
    "<img src=\"data/nb_figs/one_hot_encoding_word2vec.png\" width=\"600\"/>\n",
    "\n",
    "Há duas arquiteturas possíveis para se obter os `embeddings` word2vec. Uma delas é chamada de `CBoW` (_Continuous Bag of Words_) e outra é chamada de `Skip gram`. Aqui, vamos focar no `Skip gram`, que considera como input o vetor da palavra central da janela, e como output, os vetores do contexto. O objetivo do algoritmo é aprender os pesos da _hidden layer_, de forma que as probabilidades finais sejam condizentes com as co-ocorrências das palavras em nosso _corpus_ de documentos.\n",
    "\n",
    "<img src=\"data/nb_figs/nn_word2vec_large.png\" width=\"800\"/>\n",
    "\n",
    "Ao final do treinamento, a matriz correspondente à _hidden layer_, com 10 mil (tamanho do vocabulário) linhas e 300 (quantidade de dimensões do _embedding_) colunas será tal que cada linha representará o embedding de uma palavra do vocabulário.\n",
    "\n",
    "Para saber mais sobre `word2vec`, leia em:\n",
    "\n",
    "* http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "* https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/\n",
    "* https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade entre ingredientes - uma aplicação do _word2vec_ a um dataset de receitas\n",
    "\n",
    "O dataset utilizado é um dos datasets do site [Recipe box](https://eightportions.com/datasets/Recipes).\n",
    "\n",
    "A ideia é treinar um modelo `word2vec` usando a biblioteca [gensim](https://radimrehurek.com/gensim/index.html) e depois construirmos uma aplicação pela qual seja possível obter uma lista dos ingredientes mais similares a um determinado ingrediente. Vamos tentar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/datasets/recipes/recipes_raw_nosource_ar.json') as f:\n",
    "    recipes_list = list(json.load(f).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(recipes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['instructions'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "      <th>picture_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slow Cooker Chicken and Dumplings</td>\n",
       "      <td>[4 skinless, boneless chicken breast halves AD...</td>\n",
       "      <td>Place the chicken, butter, soup, and onion in ...</td>\n",
       "      <td>55lznCYBbs2mT8BTx6BTkLhynGHzM.S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome Slow Cooker Pot Roast</td>\n",
       "      <td>[2 (10.75 ounce) cans condensed cream of mushr...</td>\n",
       "      <td>In a slow cooker, mix cream of mushroom soup, ...</td>\n",
       "      <td>QyrvGdGNMBA2lDdciY0FjKu.77MM0Oe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown Sugar Meatloaf</td>\n",
       "      <td>[1/2 cup packed brown sugar ADVERTISEMENT, 1/2...</td>\n",
       "      <td>Preheat oven to 350 degrees F (175 degrees C)....</td>\n",
       "      <td>LVW1DI0vtlCrpAhNSEQysE9i/7rJG56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best Chocolate Chip Cookies</td>\n",
       "      <td>[1 cup butter, softened ADVERTISEMENT, 1 cup w...</td>\n",
       "      <td>Preheat oven to 350 degrees F (175 degrees C)....</td>\n",
       "      <td>0SO5kdWOV94j6EfAVwMMYRM3yNN8eRi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homemade Mac and Cheese Casserole</td>\n",
       "      <td>[8 ounces whole wheat rotini pasta ADVERTISEME...</td>\n",
       "      <td>Preheat oven to 350 degrees F. Line a 2-quart ...</td>\n",
       "      <td>YCnbhplMgiraW4rUXcybgSEZinSgljm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "0  Slow Cooker Chicken and Dumplings   \n",
       "1      Awesome Slow Cooker Pot Roast   \n",
       "2               Brown Sugar Meatloaf   \n",
       "3        Best Chocolate Chip Cookies   \n",
       "4  Homemade Mac and Cheese Casserole   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  [4 skinless, boneless chicken breast halves AD...   \n",
       "1  [2 (10.75 ounce) cans condensed cream of mushr...   \n",
       "2  [1/2 cup packed brown sugar ADVERTISEMENT, 1/2...   \n",
       "3  [1 cup butter, softened ADVERTISEMENT, 1 cup w...   \n",
       "4  [8 ounces whole wheat rotini pasta ADVERTISEME...   \n",
       "\n",
       "                                        instructions  \\\n",
       "0  Place the chicken, butter, soup, and onion in ...   \n",
       "1  In a slow cooker, mix cream of mushroom soup, ...   \n",
       "2  Preheat oven to 350 degrees F (175 degrees C)....   \n",
       "3  Preheat oven to 350 degrees F (175 degrees C)....   \n",
       "4  Preheat oven to 350 degrees F. Line a 2-quart ...   \n",
       "\n",
       "                      picture_link  \n",
       "0  55lznCYBbs2mT8BTx6BTkLhynGHzM.S  \n",
       "1  QyrvGdGNMBA2lDdciY0FjKu.77MM0Oe  \n",
       "2  LVW1DI0vtlCrpAhNSEQysE9i/7rJG56  \n",
       "3  0SO5kdWOV94j6EfAVwMMYRM3yNN8eRi  \n",
       "4  YCnbhplMgiraW4rUXcybgSEZinSgljm  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_table = str.maketrans({key: ' ' for key in string.punctuation}) \n",
    "def remove_punctuation(text):\n",
    "    return text.translate(translation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_regex = re.compile(r'[0-9]')\n",
    "def remove_digits(text):\n",
    "    return digits_regex.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text.split() if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_normalized_tokens(text):\n",
    "    text = text.lower()\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_digits(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['norm_instructions'] = df['instructions'].apply(text_to_normalized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['norm_instructions'].str.join('').str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "      <th>picture_link</th>\n",
       "      <th>norm_instructions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slow Cooker Chicken and Dumplings</td>\n",
       "      <td>[4 skinless, boneless chicken breast halves AD...</td>\n",
       "      <td>Place the chicken, butter, soup, and onion in ...</td>\n",
       "      <td>55lznCYBbs2mT8BTx6BTkLhynGHzM.S</td>\n",
       "      <td>[place, chicken, butter, soup, onion, slow, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome Slow Cooker Pot Roast</td>\n",
       "      <td>[2 (10.75 ounce) cans condensed cream of mushr...</td>\n",
       "      <td>In a slow cooker, mix cream of mushroom soup, ...</td>\n",
       "      <td>QyrvGdGNMBA2lDdciY0FjKu.77MM0Oe</td>\n",
       "      <td>[slow, cooker, mix, cream, mushroom, soup, dry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown Sugar Meatloaf</td>\n",
       "      <td>[1/2 cup packed brown sugar ADVERTISEMENT, 1/2...</td>\n",
       "      <td>Preheat oven to 350 degrees F (175 degrees C)....</td>\n",
       "      <td>LVW1DI0vtlCrpAhNSEQysE9i/7rJG56</td>\n",
       "      <td>[preheat, oven, degrees, f, degrees, c, lightl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best Chocolate Chip Cookies</td>\n",
       "      <td>[1 cup butter, softened ADVERTISEMENT, 1 cup w...</td>\n",
       "      <td>Preheat oven to 350 degrees F (175 degrees C)....</td>\n",
       "      <td>0SO5kdWOV94j6EfAVwMMYRM3yNN8eRi</td>\n",
       "      <td>[preheat, oven, degrees, f, degrees, c, cream,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homemade Mac and Cheese Casserole</td>\n",
       "      <td>[8 ounces whole wheat rotini pasta ADVERTISEME...</td>\n",
       "      <td>Preheat oven to 350 degrees F. Line a 2-quart ...</td>\n",
       "      <td>YCnbhplMgiraW4rUXcybgSEZinSgljm</td>\n",
       "      <td>[preheat, oven, degrees, f, line, quart, casse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "0  Slow Cooker Chicken and Dumplings   \n",
       "1      Awesome Slow Cooker Pot Roast   \n",
       "2               Brown Sugar Meatloaf   \n",
       "3        Best Chocolate Chip Cookies   \n",
       "4  Homemade Mac and Cheese Casserole   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  [4 skinless, boneless chicken breast halves AD...   \n",
       "1  [2 (10.75 ounce) cans condensed cream of mushr...   \n",
       "2  [1/2 cup packed brown sugar ADVERTISEMENT, 1/2...   \n",
       "3  [1 cup butter, softened ADVERTISEMENT, 1 cup w...   \n",
       "4  [8 ounces whole wheat rotini pasta ADVERTISEME...   \n",
       "\n",
       "                                        instructions  \\\n",
       "0  Place the chicken, butter, soup, and onion in ...   \n",
       "1  In a slow cooker, mix cream of mushroom soup, ...   \n",
       "2  Preheat oven to 350 degrees F (175 degrees C)....   \n",
       "3  Preheat oven to 350 degrees F (175 degrees C)....   \n",
       "4  Preheat oven to 350 degrees F. Line a 2-quart ...   \n",
       "\n",
       "                      picture_link  \\\n",
       "0  55lznCYBbs2mT8BTx6BTkLhynGHzM.S   \n",
       "1  QyrvGdGNMBA2lDdciY0FjKu.77MM0Oe   \n",
       "2  LVW1DI0vtlCrpAhNSEQysE9i/7rJG56   \n",
       "3  0SO5kdWOV94j6EfAVwMMYRM3yNN8eRi   \n",
       "4  YCnbhplMgiraW4rUXcybgSEZinSgljm   \n",
       "\n",
       "                                   norm_instructions  \n",
       "0  [place, chicken, butter, soup, onion, slow, co...  \n",
       "1  [slow, cooker, mix, cream, mushroom, soup, dry...  \n",
       "2  [preheat, oven, degrees, f, degrees, c, lightl...  \n",
       "3  [preheat, oven, degrees, f, degrees, c, cream,...  \n",
       "4  [preheat, oven, degrees, f, line, quart, casse...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do _word2vec_\n",
    "\n",
    "**Tarefa:** Treine um modelo word2vec usando os dados da coluna `ingredients` (`words_list`)\n",
    "\n",
    "1. crie uma variável chamada `word_list` que é uma lista com os ingredientes\n",
    "\n",
    "2. defina as variáveis `size` (tamanho do embedding) e `window` (tamanho da janela que deslizará pelas listas de palavras\n",
    "\n",
    "3. faça o treinamento do word2vec, passando como parâmetros `word_list`, `size=size`, `window=window`, `min_count=1` e `workers=4` (vc pode aumentar esse número ou diminuir de acordo com a quantidade de cpus que vc tiver disponível)\n",
    "\n",
    "Dica: Leia a documentação sobre a classe `Word2Vec`\n",
    "\n",
    "<!-- \n",
    "words_list = df['norm_instruction'].tolist()\n",
    "size = 300\n",
    "window = 5\n",
    "model = Word2Vec(words_list, size=size, window=window, min_count=1, workers=4)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mns_exponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhashfxn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnull_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msorted_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_final_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
       "\n",
       "Once you're finished training a model (=no more updates, only querying)\n",
       "store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
       "\n",
       "The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
       ":meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
       "\n",
       "The trained word vectors can also be stored/loaded from a format compatible with the\n",
       "original word2vec implementation via `self.wv.save_word2vec_format`\n",
       "and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
       "\n",
       "Some important attributes are the following:\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
       "    This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
       "    directly to query those embeddings in various ways. See the module level docstring for examples.\n",
       "\n",
       "vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
       "    This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
       "    Besides keeping track of all unique words, this object provides extra functionality, such as\n",
       "    constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
       "\n",
       "trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
       "    This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
       "    network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
       "    a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
       "    (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Parameters\n",
       "----------\n",
       "sentences : iterable of iterables, optional\n",
       "    The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
       "    consider an iterable that streams the sentences directly from disk/network.\n",
       "    See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
       "    or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
       "    See also the `tutorial on data streaming in Python\n",
       "    <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
       "    If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
       "    in some other way.\n",
       "corpus_file : str, optional\n",
       "    Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
       "    You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
       "    `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
       "size : int, optional\n",
       "    Dimensionality of the word vectors.\n",
       "window : int, optional\n",
       "    Maximum distance between the current and predicted word within a sentence.\n",
       "min_count : int, optional\n",
       "    Ignores all words with total frequency lower than this.\n",
       "workers : int, optional\n",
       "    Use these many worker threads to train the model (=faster training with multicore machines).\n",
       "sg : {0, 1}, optional\n",
       "    Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
       "hs : {0, 1}, optional\n",
       "    If 1, hierarchical softmax will be used for model training.\n",
       "    If 0, and `negative` is non-zero, negative sampling will be used.\n",
       "negative : int, optional\n",
       "    If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
       "    should be drawn (usually between 5-20).\n",
       "    If set to 0, no negative sampling is used.\n",
       "ns_exponent : float, optional\n",
       "    The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
       "    to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
       "    than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
       "    More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
       "    other values may perform better for recommendation applications.\n",
       "cbow_mean : {0, 1}, optional\n",
       "    If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
       "alpha : float, optional\n",
       "    The initial learning rate.\n",
       "min_alpha : float, optional\n",
       "    Learning rate will linearly drop to `min_alpha` as training progresses.\n",
       "seed : int, optional\n",
       "    Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
       "    the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
       "    you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
       "    from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
       "    use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
       "max_vocab_size : int, optional\n",
       "    Limits the RAM during vocabulary building; if there are more unique\n",
       "    words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
       "    Set to `None` for no limit.\n",
       "max_final_vocab : int, optional\n",
       "    Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
       "    min_count is more than the calculated min_count, the specified min_count will be used.\n",
       "    Set to `None` if not required.\n",
       "sample : float, optional\n",
       "    The threshold for configuring which higher-frequency words are randomly downsampled,\n",
       "    useful range is (0, 1e-5).\n",
       "hashfxn : function, optional\n",
       "    Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
       "iter : int, optional\n",
       "    Number of iterations (epochs) over the corpus.\n",
       "trim_rule : function, optional\n",
       "    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
       "    be trimmed away, or handled using the default (discard if word count < min_count).\n",
       "    Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
       "    or a callable that accepts parameters (word, count, min_count) and returns either\n",
       "    :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
       "    The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
       "    model.\n",
       "\n",
       "    The input parameters are of the following types:\n",
       "        * `word` (str) - the word we are examining\n",
       "        * `count` (int) - the word's frequency count in the corpus\n",
       "        * `min_count` (int) - the minimum count threshold.\n",
       "sorted_vocab : {0, 1}, optional\n",
       "    If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
       "    See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
       "batch_words : int, optional\n",
       "    Target size (in words) for batches of examples passed to worker threads (and\n",
       "    thus cython routines).(Larger batches will be passed if individual\n",
       "    texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
       "compute_loss: bool, optional\n",
       "    If True, computes and stores loss value which can be retrieved using\n",
       "    :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
       "callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
       "    Sequence of callbacks to be executed at specific stages during training.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> from gensim.models import Word2Vec\n",
       "    >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
       "    >>> model = Word2Vec(sentences, min_count=1)\n",
       "\u001b[0;31mFile:\u001b[0m           ~/conda/lib/python3.7/site-packages/gensim/models/word2vec.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = df['norm_instructions'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['place', 'chicken', 'butter', 'soup', 'onion', 'slow', 'cooker', 'fill', 'enough', 'water', 'cover', 'cover', 'cook', 'hours', 'high', 'minutes', 'serving', 'place', 'torn', 'biscuit', 'dough', 'slow', 'cooker', 'cook', 'dough', 'longer', 'raw', 'center'], ['slow', 'cooker', 'mix', 'cream', 'mushroom', 'soup', 'dry', 'onion', 'soup', 'mix', 'water', 'place', 'pot', 'roast', 'slow', 'cooker', 'coat', 'soup', 'mixture', 'cook', 'high', 'setting', 'hours', 'low', 'setting', 'hours']]\n"
     ]
    }
   ],
   "source": [
    "print(words_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 300\n",
    "window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.6 s, sys: 188 ms, total: 32.8 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(words_list, size=size, window=window, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você quiser salvar o modelo, você precisa apenas utilizar o método `save`, passando como parâmetro o local em que deseja que o modelo seja salvo.\n",
    "\n",
    "Por exemplo:\n",
    "```python\n",
    "model.save('data/gensim.model')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade entre vetores\n",
    "\n",
    "Em modelos vetoriais de linguagem, em geral, utiliza-se a similaridade de cosseno como medida de similaridade entre dois vetores, já que ela captura a noção de que vetores apontando para a mesma direção são próximos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_vec = model.wv['lime']\n",
    "lemon_vec = model.wv['lemon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_between_vec(vec1, vec2):\n",
    "    return 1 - cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.811663031578064"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_between_vec(lime_vec, lemon_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Termos mais comuns\n",
    "\n",
    "Vamos ver quais são os termos mais comuns do dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = chain.from_iterable(df['norm_instructions'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('minutes', 60807),\n",
       " ('heat', 44501),\n",
       " ('stir', 40642),\n",
       " ('degrees', 40360),\n",
       " ('bowl', 32988),\n",
       " ('oven', 32953),\n",
       " ('mixture', 32805),\n",
       " ('cook', 28047),\n",
       " ('medium', 25904),\n",
       " ('add', 25235),\n",
       " ('large', 25079),\n",
       " ('place', 23035),\n",
       " ('salt', 22842),\n",
       " ('mix', 22016),\n",
       " ('sugar', 21408),\n",
       " ('baking', 21166),\n",
       " ('water', 21160),\n",
       " ('f', 20765),\n",
       " ('pepper', 20350),\n",
       " ('c', 19586)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(all_words).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os mais próximos\n",
    "\n",
    "Um método legal do objeto `Word2VecKeyedVectors` é o `most_similar`, que retorna as palavras mais similares a uma determinada palavra. Note que podemos modificar a quantidade de itens retornados, colocando um valor para parâmetro `topn` (por padrão, ele é 10).\n",
    "\n",
    "**Tarefa:** brinque até ficar satisfeito.\n",
    "\n",
    "As relações fazem sentido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Find the top-N most similar words.\n",
       "Positive words contribute positively towards the similarity, negative words negatively.\n",
       "\n",
       "This method computes cosine similarity between a simple mean of the projection\n",
       "weight vectors of the given words and the vectors for each word in the model.\n",
       "The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
       "word2vec implementation.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "positive : list of str, optional\n",
       "    List of words that contribute positively.\n",
       "negative : list of str, optional\n",
       "    List of words that contribute negatively.\n",
       "topn : int or None, optional\n",
       "    Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
       "    then similarities for all words are returned.\n",
       "restrict_vocab : int, optional\n",
       "    Optional integer which limits the range of vectors which\n",
       "    are searched for most-similar values. For example, restrict_vocab=10000 would\n",
       "    only check the first 10000 word vectors in the vocabulary order. (This may be\n",
       "    meaningful if you've sorted the vocabulary by descending frequency.)\n",
       "\n",
       "Returns\n",
       "-------\n",
       "list of (str, float) or numpy.array\n",
       "    When `topn` is int, a sequence of (word, similarity) is returned.\n",
       "    When `topn` is None, then similarities for all words are returned as a\n",
       "    one-dimensional numpy array with the size of the vocabulary.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/conda/lib/python3.7/site-packages/gensim/models/keyedvectors.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?model.wv.most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quinoa', 0.6486576795578003),\n",
       " ('lentils', 0.6182992458343506),\n",
       " ('barley', 0.5376075506210327),\n",
       " ('couscous', 0.49438077211380005),\n",
       " ('farro', 0.49112915992736816),\n",
       " ('millet', 0.486141562461853),\n",
       " ('absorbed', 0.48095762729644775),\n",
       " ('noodles', 0.4803483486175537),\n",
       " ('vegetables', 0.4523196220397949),\n",
       " ('saffron', 0.4370798170566559)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('rice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização das relações entre os ingredientes\n",
    "\n",
    "Vamos agora construir funções que permitem:\n",
    "\n",
    "1. buscar o nome de um ingrediente\n",
    "2. retornar os termos mais próximos (que não são ele mesmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = set(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa:** Complete a função abaixo, que dado um termo (`word`), retorna os `n` termos mais similares. Além disso, se a palavra não está no vocabulário, imprime uma mensagem que avisa o usuário que a palavra não existe no vocabulário.\n",
    "\n",
    "Lembre-se de passar o parâmetro `topn` (`topn=n`) para o método `model.wv.most_similar` para retornar os `n` termos.\n",
    "\n",
    "<!-- \n",
    "def get_similar(word, n):\n",
    "    if word not in VOCAB:\n",
    "        print(f'A palavra \"{word}\" não está em nosso vocabulário!')\n",
    "        exit(1)\n",
    "    \n",
    "    most_similar_words = []\n",
    "    for similar_word, distance in model.wv.most_similar(word, n=n+1):\n",
    "        most_similar_words.append(similar_word)\n",
    "        \n",
    "    return most_similar_words\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar(word, n=10):\n",
    "    if word not in VOCAB:\n",
    "        print(f'A palavra \"{word}\" não está em nosso vocabulário!')\n",
    "        exit(1)\n",
    "    \n",
    "    most_similar_words = []\n",
    "    for similar_word, distance in model.wv.most_similar(word, topn=n+1):\n",
    "        most_similar_words.append(similar_word)\n",
    "        \n",
    "    return most_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_text(text, max_display=15):\n",
    "    words_with_text = [word for word in VOCAB if text.lower() in word][:max_display]\n",
    "    if text in words_with_text:\n",
    "        words_with_text = [text] + [word for word in words_with_text if text != word]\n",
    "    return words_with_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rosemary']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_text('rosemary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thyme',\n",
       " 'sage',\n",
       " 'marjoram',\n",
       " 'tarragon',\n",
       " 'oregano',\n",
       " 'sprigs',\n",
       " 'basil',\n",
       " 'parsley',\n",
       " 'savory',\n",
       " 'paprika',\n",
       " 'dill']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar('rosemary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa bônus:** descubra a similaridade entre uma receita e outra receita através dos embeddings que treinamos acima. Isso envolve utilizar um embedding para a instrução (a nossa \"sentença\", que é um conjunto de palavras).\n",
    "\n",
    "Veja exemplos da criação de embeddings para sentença a partir dos embeddings da palavra [neste post](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/).\n",
    "\n",
    "Após a criação dos embeddings das receitas, basta comparará-las!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
