{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula #26 – Processamento de Linguagem Natural & Análise de Sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "Já vimos antes que é possível transformar um texto em _features_ numéricas. Uma sofisticação do método _Bag of words_ é incorporar o contexto das palavras vizinhas nessas _features_ (é comum chamar o vetor de _features_ numéricas de _embedding_).\n",
    "\n",
    "Imagine que nossa janela de contexto (context window) tem tamanho 5 (2 palavras _antes_ e 2 palavras _depois_ da palavra _central_).\n",
    "\n",
    "Então, se a frase fosse `The quick brown fox jumps over the lazy dog`, teríamos as seguintes janelas:\n",
    "\n",
    "<img src=\"data/nb_figs/windows_word2vec.png\" width=\"600\"/>\n",
    "\n",
    "Para cada uma das janelas formadas, temos o vetor correspondente a elas (usando o _Bag of words_ binário - com apenas 0s e 1s; também chamado de `one-hot encoding`):\n",
    "\n",
    "<img src=\"data/nb_figs/one_hot_encoding_word2vec.png\" width=\"600\"/>\n",
    "\n",
    "Há duas arquiteturas possíveis para se obter os `embeddings` word2vec. Uma delas é chamada de `CBoW` (_Continuous Bag of Words_) e outra é chamada de `Skip gram`. Aqui, vamos focar no `Skip gram`, que considera como input o vetor da palavra central da janela, e como output, os vetores do contexto. O objetivo do algoritmo é aprender os pesos da _hidden layer_, de forma que as probabilidades finais sejam condizentes com as co-ocorrências das palavras em nosso _corpus_ de documentos.\n",
    "\n",
    "<img src=\"data/nb_figs/nn_word2vec_large.png\" width=\"800\"/>\n",
    "\n",
    "Ao final do treinamento, a matriz correspondente à _hidden layer_, com 10 mil (tamanho do vocabulário) linhas e 300 (quantidade de dimensões do _embedding_) colunas será tal que cada linha representará o embedding de uma palavra do vocabulário.\n",
    "\n",
    "Para saber mais sobre `word2vec`, leia em:\n",
    "\n",
    "* http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "* https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/\n",
    "* https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade entre ingredientes - uma aplicação do _word2vec_ a um dataset de receitas\n",
    "\n",
    "O dataset utilizado é um dos datasets do site [Recipe box](https://eightportions.com/datasets/Recipes).\n",
    "\n",
    "A ideia é treinar um modelo `word2vec` usando a biblioteca [gensim](https://radimrehurek.com/gensim/index.html) e depois construirmos uma aplicação pela qual seja possível obter uma lista dos ingredientes mais similares a um determinado ingrediente. Vamos tentar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/datasets/recipes/recipes_raw_nosource_ar.json') as f:\n",
    "    recipes_list = list(json.load(f).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(recipes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['instructions'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização da coluna `instructions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_table = str.maketrans({key: ' ' for key in string.punctuation}) \n",
    "def remove_punctuation(text):\n",
    "    return text.translate(translation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_regex = re.compile(r'[0-9]')\n",
    "def remove_digits(text):\n",
    "    return digits_regex.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text.split() if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_normalized_tokens(text):\n",
    "    text = text.lower()\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_digits(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['norm_instructions'] = df['instructions'].apply(text_to_normalized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['norm_instructions'].str.join('').str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do _word2vec_\n",
    "\n",
    "**Tarefa:** Treine um modelo word2vec usando os dados da coluna `ingredients` (`words_list`)\n",
    "\n",
    "1. crie uma variável chamada `word_list` que é uma lista com os ingredientes\n",
    "\n",
    "2. defina as variáveis `size` (tamanho do embedding) e `window` (tamanho da janela que deslizará pelas listas de palavras\n",
    "\n",
    "3. faça o treinamento do word2vec, passando como parâmetros `word_list`, `size=size`, `window=window`, `min_count=1` e `workers=4` (vc pode aumentar esse número ou diminuir de acordo com a quantidade de cpus que vc tiver disponível)\n",
    "\n",
    "Dica: Leia a documentação sobre a classe `Word2Vec`\n",
    "\n",
    "<!-- \n",
    "words_list = df['norm_instruction'].tolist()\n",
    "size = 300\n",
    "window = 5\n",
    "model = Word2Vec(words_list, size=size, window=window, min_count=1, workers=4)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "words_list = ###\n",
    "\n",
    "# 2\n",
    "size = ###\n",
    "window = ###\n",
    "\n",
    "# 3\n",
    "model = Word2Vec(###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você quiser salvar o modelo, você precisa apenas utilizar o método `save`, passando como parâmetro o local em que deseja que o modelo seja salvo.\n",
    "\n",
    "Por exemplo:\n",
    "```python\n",
    "model.save('data/gensim.model')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade entre vetores\n",
    "\n",
    "Em modelos vetoriais de linguagem, em geral, utiliza-se a similaridade de cosseno como medida de similaridade entre dois vetores, já que ela captura a noção de que vetores apontando para a mesma direção são próximos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_vec = model.wv['lime']\n",
    "lemon_vec = model.wv['lemon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_between_vec(vec1, vec2):\n",
    "    return 1 - cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_between_vec(lime_vec, lemon_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Termos mais comuns\n",
    "\n",
    "Vamos ver quais são os termos mais comuns do dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = chain.from_iterable(df['norm_instructions'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(all_words).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os mais próximos\n",
    "\n",
    "Um método legal do objeto `Word2VecKeyedVectors` é o `most_similar`, que retorna as palavras mais similares a uma determinada palavra. Note que podemos modificar a quantidade de itens retornados, colocando um valor para parâmetro `topn` (por padrão, ele é 10).\n",
    "\n",
    "**Tarefa:** brinque até ficar satisfeito.\n",
    "\n",
    "As relações fazem sentido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?model.wv.most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('rice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização das relações entre os ingredientes\n",
    "\n",
    "Vamos agora construir funções que permitem:\n",
    "\n",
    "1. buscar o nome de um ingrediente\n",
    "2. retornar os termos mais próximos (que não são ele mesmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = set(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa:** Complete a função abaixo, que dado um termo (`word`), retorna os `n` termos mais similares. Além disso, se a palavra não está no vocabulário, imprime uma mensagem que avisa o usuário que a palavra não existe no vocabulário.\n",
    "\n",
    "Lembre-se de passar o parâmetro `topn` (`topn=n`) para o método `model.wv.most_similar` para retornar os `n` termos.\n",
    "\n",
    "<!-- \n",
    "def get_similar(word, n=10):\n",
    "    if word not in VOCAB:\n",
    "        print(f'A palavra \"{word}\" não está em nosso vocabulário!')\n",
    "        exit(1)\n",
    "    \n",
    "    most_similar_words = []\n",
    "    for similar_word, distance in model.wv.most_similar(word, topn=n+1):\n",
    "        most_similar_words.append(similar_word)\n",
    "        \n",
    "    return most_similar_words\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar(word, n=10):\n",
    "    if word not in VOCAB:\n",
    "        print(###\n",
    "        exit(1)\n",
    "    \n",
    "    most_similar_words = []\n",
    "    for similar_word, distance in model.wv.most_similar(###):\n",
    "        most_similar_words.append(###\n",
    "        \n",
    "    return most_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_text(text, max_display=15):\n",
    "    words_with_text = [word for word in VOCAB if text.lower() in word][:max_display]\n",
    "    if text in words_with_text:\n",
    "        words_with_text = [text] + [word for word in words_with_text if text != word]\n",
    "    return words_with_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_text('rosemary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar('rosemary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa bônus:** descubra a similaridade entre uma receita e outra receita através dos embeddings que treinamos acima. Isso envolve utilizar um embedding para a instrução (a nossa \"sentença\", que é um conjunto de palavras).\n",
    "\n",
    "Veja exemplos da criação de embeddings para sentença a partir dos embeddings da palavra [neste post](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/).\n",
    "\n",
    "Após a criação dos embeddings das receitas, basta comparará-las!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
