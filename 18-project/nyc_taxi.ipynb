{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York City Taxi Trip Duration\n",
    "<hr>\n",
    "\n",
    "![](imgs/taxi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução <a id=\"intro\"></a>\n",
    "\n",
    "O dataset dessa aula foi extraído da competição do Kaggle [New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration/overview). Nessa competição, o objetivo era prever a duração de uma viagem de táxi em Nova York, usando como dados as coordenadas geográficas do ponto de partida e de chegada e o momento da partida.\n",
    "\n",
    "Mas peraí, por que prever a duração de uma viagem quando a gente já sabe o ponto de chegada?? Nem toda competição do Kaggle faz sentido na vida real, e foi feito até um disclaimer na descrição dos dados:\n",
    "\n",
    "*Disclaimer: The decision was made to not remove dropoff coordinates from the dataset order to provide an expanded set of variables to use in Kernels.*\n",
    "\n",
    "Mas há uma situação em que a competição faz muito sentido! Quando você pede um Uber, coloca logo o ponto de partida e o de chegada, e o aplicativo te dá uma previsão do tempo da viagem. É nesse cenário que vamos pensar!\n",
    "\n",
    "\n",
    "**Importante**: O dataset que usaremos nessa aula não é exatamente o da competição. Foram adicionadas duas colunas com a região e o bairro de Nova York em que a corrida começou/terminou (informações obtidas [aqui](https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm)). Além disso, para deixar a execução mais rápida, usaremos uma versão menor do dataset. Se quise ver como os dados foram preparados veja o notebook [prepare_dataset.ipynb](prepare_dataset.ipynb).\n",
    "\n",
    "A métrica utilizada na competição é a **Root Mean Squared Logarithmic Error** (RMSLE), definida como\n",
    "\n",
    "$$RMSLE = \\sqrt{\\frac1n \\sum_{i=1}^n\\big(\\log(p_i+1)-\\log(a_i+i)\\big)^2}$$\n",
    "\n",
    "onde $p_i$ é a predição para o exemplo $i$ e $a_i$ o ground-truth. Essa métrica motiva a transformação do target com a função $f(x) =\\log(x+1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importando as bibliotecas <a id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "from scipy.stats import probplot\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Leitura dos dados <a id=\"read\"></a>\n",
    "\n",
    "Pra começar, leia os arquivos '../data/train_with_nta.csv' e '../data/test_with_nta.csv', e em seguida dê um .head(), .info() e .describe() no train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/prepared_train.csv')\n",
    "test = pd.read_csv('../data/prepared_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head, describe, info\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que as colunas **pickup_datetime** e **dropoff_datetime** estão como object. Faça a conversão para **datetime** do pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter as colunas para datetime\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você já souber que alguma coluna é datetime, pode fazer a conversão na hora da leitura, dessa forma:\n",
    "\n",
    "```python\n",
    "train = pd.read_csv('../data/prepared_train.csv', parse_dates=['pickup_datetime', 'dropoff_datetime'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheque se os datasets possuem algum **NaN**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheque a presença de missing values no treino e no teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tudo carregado na memória! Agora vamos começar a EDA. Você notou alguma coisa estranha no train.describe()? Plote a distribuição do target no train usando **sns.displot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar distribuição do target no treino\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plote um boxplot do target no treino, usando **sns.boxplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar boxplot do target no treino\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual o valor máximo da trip_duration no treino, em horas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# achar valor maximo de trip_duration\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantas viagens duraram mais que 4h?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contar número de viagens com mais de 4h\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remova as viagens com duração anormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover outliers no trip_duration\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plote novamente a distribuição do target e o boxplot (coloque o eixo x em horas ao invés de segundos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar novamente distribuição e boxplot do target no treino\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em problemas de regressão, é comum aplicar a função log(x) ou log(x+1) no target. Plote a distribuição do log do target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar a ditribuição do log do target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa distribuição parece gaussiana. Um dos jeitos de checar visualmente se seus dados seguem uma normal é fazer um [probability plot](https://en.wikipedia.org/wiki/Normal_probability_plot). Compare os plots feitos com variáveis geradas de uma distribuição normal e de uma exponencial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "_ = probplot(np.random.normal(size=1000), plot=axs[0])\n",
    "_ = probplot(np.random.exponential(size=1000), plot=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare os plots feitos com o target e com o log do target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "_ = probplot(train.trip_duration, plot=axs[0])\n",
    "_ = probplot(np.log(train.trip_duration), plot=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há algum outlier no número de passageiros? Faça um describe e um histograma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checar distribuição do número de passageiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como viram na aula de regressão linear, os táxis formam um mapa da cidade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "train.plot(kind='scatter', x='pickup_longitude', y='pickup_latitude', s=0.2, alpha=0.1, color='salmon', ax=ax)\n",
    "ax.set_facecolor('black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A figura é melhor visualizada se limitarmos os eixos x e y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "long_interval = (-74.04, -73.75)\n",
    "lat_interval = (40.63, 40.88)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "train.plot(kind='scatter', x='pickup_longitude', y='pickup_latitude', s=0.2, alpha=0.1, color='salmon', ax=ax)\n",
    "ax.set_facecolor('black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma imagem parecida também pode ser gerada coma biblioteca [Datashader](https://datashader.org/). Ela gera a imagem bem mais rápido do que o .scatter e dá muitas opções pra manipulação, não faz parte da aula, mas aqui vai o código como exemplo e a figura que ele geraria:\n",
    "\n",
    "```python\n",
    "from colorcet import fire\n",
    "import datashader as ds \n",
    "import datashader.transfer_functions as dtf\n",
    "from datashader.utils import lnglat_to_meters\n",
    "\n",
    "data = pd.read_csv('../data/train.csv')\n",
    "x, y = lnglat_to_meters(data.pickup_longitude, data.pickup_latitude)\n",
    "mercator_df = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "west, south, east, north = 40.635, -74.03, 40.86, -73.77\n",
    "bottom_left = lnglat_to_meters(south, west)\n",
    "top_right = lnglat_to_meters(north, east)\n",
    "x_range = (bottom_left[0], top_right[0])\n",
    "y_range = (bottom_left[1], top_right[1])\n",
    "\n",
    "canvas = ds.Canvas(plot_width=600, plot_height=600, x_range=x_range, y_range=y_range)\n",
    "agg = canvas.points(mercator_df, 'x', 'y')\n",
    "dtf.set_background(dtf.shade(agg, cmap=fire), 'black')\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "![](imgs/datashader.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A coluna de datetime bruta não pode ser usada diretamente como feature. Por isso, vamos separá-la em hora, mês, dia da semana e ano. Crie as colunas abaixo usando as [propriedades .dt](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.html) do pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hour'] = #\n",
    "train['day'] = #\n",
    "train['weekday'] = #\n",
    "train['month'] = #\n",
    "\n",
    "test['hour'] = #\n",
    "test['day'] = #\n",
    "test['weekday'] = #\n",
    "test['month'] = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com esses features iniciais já podemos tirar informações interessantes sobre o trânsito de Nova York. Use a função **sns.countplot**, vista na aula de regressão linear, pra plotar a distribuição do número de corridas pelas horas do dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countplot na hora\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que está por baixo dos panos do sns.countplot é um groupby. Tente reproduzir o gráfico acima usando grouby e a função de agregação **size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby na hora\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a função **sns.catplot** para plotar a média da duração das viagens a cada hora do dia (um gráfico parecido foi feito na aula de regressão linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catplot trip_duration vs hour\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tente reproduzir o gráfico acima usando groupby (não precisa das barras de erro nem das cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduzir catplot trip_duration vs hour\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual região tem mais viagens iniciadas? Responda essa pergunta com um gráfico, usando countplot ou groupby em **pickup_neighborhood**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countplot no pickup_neighborhood\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faço o mesmo para viagens finalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countplot no dropoff_neighborhood\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como visualizar a série temporal do númerod e viagens a cada dia? Uma maneira possível é utilizar a propriedade **dayoftheyear** e realizar um groupby (note a key passada para o groupby não precisa ser uma coluna do dataframe, basta ter o mesmo tamanho):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(train.pickup_datetime.dt.dayofyear).size().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas e se quisermos plotar o número de viagens a cada semana, a cada 15 dias, a cada 17 minutos? Para isso usamos o [**pd.Grouper**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Grouper.html), que nos auxilia a fazer groupby sobre datas, especificando-se a frequência. Por exemplo, o gráfico acima pode ser feito assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(pd.Grouper(key='pickup_datetime', freq='d')).size().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos plotar a contagem a cada 15 dias ou a cada semana, basta mudar o paramêtro de frequência para '15d' ou 'w':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(pd.Grouper(key='pickup_datetime', freq='15d')).size().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(pd.Grouper(key='pickup_datetime', freq='w')).size().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos, é claro, usar outras funções de agregação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(pd.Grouper(key='pickup_datetime', freq='w')).mean().trip_duration.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agrupar em mais de uma coluna (não se preocupe muito com o comando unstack):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby([pd.Grouper(key='pickup_datetime', freq='d'), 'vendor_id']).size().unstack().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vendo esses gráficos, parece ter um dia bem anormal no final de janeiro. Qual foi esse dia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar dia anormal\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pesquise no google o que aconteceu em Nova York nesse dia.\n",
    "\n",
    "Essa conclusão nos motiva a procurar features baseados nas condições climáticas nos dias de 2016. Uma possibilidade é procurar alguma API que forneça essas informações, mas felizmente participantes da competição no Kagge já fizeram esse trabalho! Vamos usar [esse dataset](https://www.kaggle.com/cabaki/knycmetars2016), que da deve estar na sua pasta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('../data/KNYC_Metars.csv', parse_dates=['Time'])\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça um gráfico da média temperatura (coluna 'Temp.') por dia no ano de 2016. Use o pd.Grouper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar o pd.Grouper pra plotar a evolução da temperatura ao longo do ano de 2016\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separe a coluna 'Time' em hora, dia, mês e ano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['year'] = #\n",
    "weather['month'] = #\n",
    "weather['day'] = #\n",
    "weather['hour'] = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtre o dataset removendo todas as colunas que não são de 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtre o dataset para o ano de 2016\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas manipulações finais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['snow'] = weather.Events.str.contains('Snow').astype(int) # converte para 1 todas as strings que contem \"Snow\"\n",
    "weather = weather[['month','day','hour','Temp.','Precip','snow','Visibility']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O merge com o dataset original pode ser feito assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, weather, on=['month', 'day', 'hour'], how='left')\n",
    "test = pd.merge(test, weather, on=['month', 'day', 'hour'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset ficou assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse merge introduz alguns **NaNs**, pois provalmente há falta de dados para algumas horas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma estratégia seria preencher cada missing value com a média naquele dia ou semana, mas vamos adotar uma estratégia mais simples: back fill, que consiste em preencher um missing value com o próximo valor não faltante na coluna.\n",
    "\n",
    "Use o método `fillna` com a estratégia 'bfill' nas colunas `Temp.`, `Precip` e `Visibility`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preencher os missing values do treino e teste com bfill\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vamos preencher os nans da coluna neve com zeros\n",
    "test.snow = test.snow.fillna(0)\n",
    "train.snow = train.snow.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering\n",
    "\n",
    "Vamos começar a criar features mais interessantes! Para começar, construa colunas com as variações de latitude de longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['delta_lat'] =  # TODO\n",
    "train['delta_long'] = # TODO\n",
    "\n",
    "test['delta_lat'] =   # TODO\n",
    "test['delta_long'] =  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature grátis:** distância real em quilômetros. Como ninguém é obrigado a conhecer a [fórmula de Haversine](https://en.wikipedia.org/wiki/Haversine_formula), que dá a distância entre dois pontos numa circunferência, este feature já está pronto aqui! A implementação igual à [dessa biblioteca](https://github.com/mapado/haversine), só que feita pra aceita numpy arrays como entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_haversine(lat1, long1, lat2, long2):\n",
    "    lat1, long1, lat2, long2 = map(np.radians, (lat1, long1, lat2, long2))\n",
    "    AVG_EARTH_RADIUS = 6371  # radio da Terra em km\n",
    "    delta_lat = lat2 - lat1\n",
    "    delta_long = long2 - long1\n",
    "    d = np.sin(delta_lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(delta_long * 0.5) ** 2\n",
    "    return 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['haversine_dist']  = np_haversine(train['pickup_latitude'],\n",
    "                                       train['pickup_longitude'],\n",
    "                                       train['dropoff_latitude'],\n",
    "                                       train['dropoff_longitude'])\n",
    "\n",
    "test['haversine_dist']  = np_haversine(test['pickup_latitude'],\n",
    "                                       test['pickup_longitude'],\n",
    "                                       test['dropoff_latitude'],\n",
    "                                       test['dropoff_longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse feature provavelmente é importante, mas será que a distância percorrida em linha reta tem uma relação tão direta com o tempo? Resposta óbvia pra quem mora em São Paulo: **não**. Use **sns.jointplot** e plote um gráfico de **haversine_dist** vs **trip_duration** (em horas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jointplot do target vs haversine\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como temos a distância em linha reta e o tempo da viagem, podemos criar uma coluna com a velocidade média do deslocamento. Essa coluna **não** poderá ser usada diretamente como feature. Por quê?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['average_speed'] = 3600 * train.haversine_dist / train.trip_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar de não poder usar essa coluna diretamente como feature, vamos usá-la para entender melhor o dataset. Use groupby para plotar a média das velocidades ao longo das horas do dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar média das velocidades ao longo do dia\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça o mesmo para os dias da semana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar média das velocidades ao longo da semana\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A informação da velocidade não está presente no teste, mas nada nos impede de utilizá-la para criar informações sobre cada região usando o conjunto de treino. Por exemplo, podemos calcular a velocidade média das viagens começando em cada região a cada hora e usar esse dado como feature! Isso pode ser feito com um groupby no código da área e na hora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(['pickup_ntacode', 'hour']).mean().average_speed.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_speed_nta_hour = train.groupby(['pickup_ntacode', 'hour']).mean().average_speed.reset_index()\n",
    "pickup_speed_nta_hour = pickup_speed_nta_hour.rename(columns={'average_speed': 'pickup_speed_nta_hour'})\n",
    "pickup_speed_nta_hour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tente fazer a mesma coisa com as regiões de dropoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoff_speed_nta_hour = train.groupby(['dropoff_ntacode', 'hour']).agg({'average_speed': 'mean'}).reset_index()\n",
    "dropoff_speed_nta_hour = dropoff_speed_nta_hour.rename(columns={'average_speed': 'dropoff_speed_nta_hour'})\n",
    "dropoff_speed_nta_hour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora basta juntar esses features com os datasets usando um pd.merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, pickup_speed_nta_hour, on=['pickup_ntacode', 'hour'], how='left')\n",
    "test = pd.merge(test, pickup_speed_nta_hour, on=['pickup_ntacode', 'hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, dropoff_speed_nta_hour, on=['dropoff_ntacode', 'hour'], how='left')\n",
    "test = pd.merge(test, dropoff_speed_nta_hour, on=['dropoff_ntacode', 'hour'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muitos táxis estão saindo de uma região A e indo para uma região B na mesma hora, pode ser um indicativo que o trânsito será mais lento. Usando o conjunto de treino, podemos construir essa tabela de \"táxis na mesma direção\" por hora. Tente construir essa tabela usando groupby e size(). Ela deve ficar mais ou menos assim:\n",
    "\n",
    "| pickup_ntacode | dropoff_ntacode | hour | same_direction_by_hour |\n",
    "|----------------|-----------------|------|------------------------|\n",
    "| A              | B               | 0    | 12                     |\n",
    "| C              | D               | 1    | 231                    |\n",
    "| E              | F               | 2    | 67                     |\n",
    "\n",
    "\n",
    "Use reset_index() depois do groupby para deixar o df nesse formato. Quando se usa a função de agregação size(), é criada uma coluna com o nome \"0\". Mude esse nome para algo mais apropriado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir a tabela acima com groupby\n",
    "# (pode mudar o nome se quiser)\n",
    "same_direction = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça o merge dessa tabela com o treino e com o teste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup do dataset antes do merge\n",
    "train_copy = train.copy()\n",
    "test_copy = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge da tabela same_direction com treino e teste (use how='left')\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assegure-se que o dataset tem o mesmo número de linhas após o merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_copy) == len(train)\n",
    "assert len(test_copy) == len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_copy, test_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confira se adição desses features criou **NaNs** no treino ou no teste, o que pode ocorrer por exemplo caso exista no teste um taxi percursso de taxi que não existe no treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conferir se agora há NaNs no treino ou no teste\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use alguma estratégia para eliminar esses NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar NaNs, caso existam\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que o trânsito é bem diferente em feriados. Vamos criar uma coluna binária `holiday` que diz se a data de pickup é ou não feriado em NY. Para isso, usaremos a biblioteca [holidays](https://github.com/dr-prodigy/python-holidays). Dê uma olhada no readme da repositório e tente criar essa coluna no treino e no teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar coluna binária de feriados\n",
    "\n",
    "train['holiday'] = # TODO\n",
    "test['holiday'] = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa sessão é livre! Tente usar uma regressão linear e um RandomForestRegressor. Será necessário usar algum encoding para as variáveis categóricas. Uma poissível alternativa é utilizar uma feature recente do scikit-learn: [ColumTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html). O ColumnTransformer possibilida a utilização de preprocessadores que atuam em colunas diferentes dentro do mesmo objeto!\n",
    "\n",
    "Por exemplo, podemos combinar em um só pré-processador um MinMaxScaler que atua nas features numéricas e um OneHotEncoder que atua nas features categóricas:\n",
    "\n",
    "```python\n",
    "numerical_features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "                      'dropoff_latitude', 'Temp.', 'Precip', 'snow', 'Visibility',\n",
    "                      'delta_lat', 'delta_long', 'haversine_dist',\n",
    "                      'pickup_speed_nta_hour', 'dropoff_speed_nta_hour']\n",
    "```\n",
    "\n",
    "```python\n",
    "categorical_features = ['store_and_fwd_flag', 'pickup_ntacode', 'dropoff_ntacode',\n",
    "                        'pickup_neighborhood', 'dropoff_neighborhood', 'vendor_id']\n",
    "```\n",
    "\n",
    "```python\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', MinMaxScaler(), numerical_features),\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "```\n",
    "\n",
    "Agora é só usar o fit_transform:\n",
    "\n",
    "```python\n",
    "X = preprocessor.fit_transform(train)\n",
    "```\n",
    "\n",
    "Esse processor também pode ser colocado em um pipeline junto com o classificador.\n",
    "\n",
    "\n",
    "Quando for utilizar o .fit do seu modelo, utilize a transformação np.log1p no target, dessa forma:\n",
    "\n",
    "```python\n",
    "model.fit(X, np.log1p(y))\n",
    "```\n",
    "\n",
    "Para calcular o RMSLE:\n",
    "\n",
    "```python\n",
    "RMSLE = np.sqrt(mean_squared_log_error(y_pred, np.exp(y_pred) - 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIG TODO\n",
    "# fitar regressão logística/random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obrigado!\n",
    "\n",
    "![](imgs/taxi_ending.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
